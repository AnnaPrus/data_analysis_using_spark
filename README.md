## Data Analysis Using Spark

## Technology Used
- Python: Primary programming language
- Apache Spark (PySpark): Distributed data processing and analytics framework
- Spark SQL: SQL querying interface for structured data within Spark
- Jupyter Notebook/IPython: Interactive coding and data analysis environment
- findspark & wget: Environment integration and dataset download

## Skills Showcased
- Data ingestion and cleaning
- Defining and applying explicit data schemas
- SQL querying of large datasets with Spark SQL
- Data transformation using PySpark DataFrame API
- Aggregation, filtering, grouping, and sorting in distributed systems
- Data pipeline design for analytics workflows
- String manipulation (including extracting characters and joining strings)
- Performing self-joins and advanced business logic computations

## The Task
- Load a structured CSV dataset of employee information
- Define and apply a custom schema for robust data representation
- Build a Spark DataFrame for all subsequent analytics
- Carry out a series of business-motivated analytics tasks, such as:
  - Calculating average and total salary by department
  - Filtering employees based on department or name characteristics
  - Adding computed columns (e.g., salary with bonus)
  - Performing group-bys, self-joins, and aggregation operations
  - Sorting and segmenting employee data for reporting

## Output
- Query-ready, analysis-rich DataFrames with results
- All intermediate and final results are printed directly in the Jupyter Notebook for inspection or further use in business reports and dashboards
  
